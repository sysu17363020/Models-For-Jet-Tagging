{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aeaf81-607b-4d55-9e77-be267ac08479",
   "metadata": {},
   "source": [
    "# L-GATr Quickstart\n",
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/heidelberg-hepml/lgatr/blob/main/examples/demo_lgatr.ipynb)\n",
    "\n",
    "In this tutorial, we give a quick introduction for how to use LGATr. LGATr is a Lorentz-equivariant transformer for applications in high-energy physics and other domains where Lorentz symmetry is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aeaa9b-a47d-4467-825e-b1731964bd17",
   "metadata": {},
   "source": [
    "`LGATr` is build on geometric algebra representations. The idea is to unify scalars, vectors as well as certain higher-order objects (bivectors, axial vectors, pseudoscalars) into so-called multivectors. Concretely, the input data is embedded into multivectors, then processed by the architecture while maintaining this form, before finally the relevant data is extracted from the output multivector. For the case of the Lorentz group, a multivector is a 16-dimensional object of the form $(s, v^0, v^1, v^2, v^3, \\dots )$ with a scalar $s$ and a vector $v^\\mu$. We add a seperate chain of scalar channels to formally allow for a smooth transition to non-equivariant transformers, which would only have scalar channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58232c6b-85e1-4e48-857b-24995368ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the lgatr package\n",
    "%pip install lgatr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94a506-afb2-47ad-9c44-dbdf7f1081f1",
   "metadata": {},
   "source": [
    "After importing the required modules, we construct a LGATr encoder module. The `attention` and `mlp` dicts organize hyperparameter information, you can find more information in `SelfAttentionConfig` and `MLPConfig` in the docs. They are arguments for the `LGATr` module, in addition to the number of incoming, outgoing and hidden multivector and scalar channels, and the number of `LGATr` blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99acac-8bd8-4c8a-a770-9f9619a3d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct LGATr module\n",
    "from lgatr import LGATr\n",
    "\n",
    "attention = dict(num_heads=2)\n",
    "mlp = dict()\n",
    "lgatr = LGATr(\n",
    "   in_mv_channels=1,\n",
    "   out_mv_channels=1,\n",
    "   hidden_mv_channels=8,\n",
    "   in_s_channels=0,\n",
    "   out_s_channels=0,\n",
    "   hidden_s_channels=16,\n",
    "   attention=attention,\n",
    "   mlp=mlp,\n",
    "   num_blocks=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5437a87b-26a2-4cba-bbee-5c50e777cbbb",
   "metadata": {},
   "source": [
    "We now test `LGATr` on toy data, e.g. a bunch of LHC events. We create particles with fixed mass and gaussian noise as momentum. The resulting four-momenta have shape `p.shape = (128, 20, 1, 4)`; for batch size 128, 20 particles per jet, 1 four-momentum per particle, and 4 numbers for the four-momentum. More generally, `LGATr` operates on objects of the shape `(batch_size, num_particles, num_channels, 16)`, while normal transformers operate on `(batch_size, num_particles, num_channels)`, without the extra 'multivector' dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739008ef-6a14-4f8e-a1d8-6e1e79edb4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# generate toy data\n",
    "import torch\n",
    "p3 = torch.randn(128, 20, 1, 3)\n",
    "mass = 1\n",
    "E = (mass**2 + (p3**2).sum(dim=-1, keepdim=True))**0.5\n",
    "p = torch.cat((E, p3), dim=-1)\n",
    "print(p.shape) # torch.Size([128, 20, 1, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0022a86-86ab-435b-8b91-9c1c12569f66",
   "metadata": {},
   "source": [
    "We have to embed the four-momenta into multivectors to process them with `LGATr`. The `lgatr` package has functions for that in `lgatr/interface`, usually one needs `embed_vector`, `embed_scalar`, `extract_vector` and `extract_scalar`. For instance, `embed_vector` puts the four-momentum at the indices 1-4 of the multivector, while setting the other indices to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6753009e-5ac9-427a-9a8e-1c258c05835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "from lgatr.interface import embed_vector, extract_scalar\n",
    "multivector = embed_vector(p)\n",
    "print(multivector.shape) # torch.Size([128, 20, 1, 16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05b73c-69ca-4adc-aec4-45f79cf0a17f",
   "metadata": {},
   "source": [
    "We can now process the multivector with the `LGATr` architecture! It returns another multivector, from which we can extract the component that we want -- for instance the scalar component for a jet tagging or amplitude regression application. Depending on the task, we can also include or extract additional scalar channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e61ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 20, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "output_mv, output_s = lgatr(multivectors=multivector, scalars=None)\n",
    "out = extract_scalar(output_mv)\n",
    "print(out.shape) # torch.Size([128, 20, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53356edb-8a63-4008-91cc-781afbd74e0c",
   "metadata": {},
   "source": [
    "Thats it, now you're ready to build your own `LGATr` model! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
